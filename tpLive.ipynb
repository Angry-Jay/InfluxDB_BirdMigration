{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2j9m3kslk4",
   "metadata": {},
   "source": [
    "## Partie 0 : Introduction aux Bases de Données Time-Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kd9l5mbtwih",
   "metadata": {},
   "source": "### 0.1 Qu'est-ce qu'une donnée time-series ?\n\nUne donnée time-series (série temporelle) est une séquence de points de données indexés dans l'ordre temporel. Exemples courants :\n\n- Capteurs IoT : température, pression, humidité mesurées toutes les secondes\n- Tracking d'animaux : positions GPS avec timestamp (notre cas)\n- Finance : cours de bourse, transactions bancaires\n- Monitoring IT : métriques serveur (CPU, RAM, réseau)\n\nCaractéristiques clés :\n- Chaque point a un timestamp précis\n- Les données arrivent généralement dans l'ordre chronologique\n- Souvent volumineuses (millions de points)\n- Les requêtes portent fréquemment sur des intervalles de temps"
  },
  {
   "cell_type": "markdown",
   "id": "qppqf89vp3e",
   "metadata": {},
   "source": "### 0.2 Pourquoi InfluxDB plutôt que MongoDB ou MySQL ?\n\n| Critère | MySQL | MongoDB | InfluxDB |\n|---------|-------|---------|----------|\n| Structure | Tables relationnelles | Documents JSON | Mesures + tags + fields + time |\n| Indexation temporelle | Index B-tree standard | Index classique | Index temporel natif (TSM) |\n| Agrégations temporelles | Lent | Acceptable | Très rapide |\n| Compression | Faible | Moyenne | Très élevée (10-20x) |\n| Cas d'usage | E-commerce, CRM | Applications web | **Monitoring, IoT, tracking** |\n\nPour notre dataset de migration d'oiseaux :\n- 89 867 points temporels → structure time-series\n- Requêtes fréquentes par intervalle de temps\n- Besoin d'agrégations temporelles (distance parcourue par jour)\n- Compression importante (données géographiques répétitives)"
  },
  {
   "cell_type": "markdown",
   "id": "v46bzb1lhek",
   "metadata": {},
   "source": "### 0.3 Concepts Clés d'InfluxDB\n\nInfluxDB organise les données différemment des bases relationnelles ou documents :\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│ MEASUREMENT (table)                                          │\n├─────────────────────────────────────────────────────────────┤\n│ Tags (indexés)          │ Fields (valeurs)    │ Timestamp   │\n│ ======================= │ =================== │ =========== │\n│ bird_id: \"91732A\"       │ latitude: 61.24     │ 2009-05-27  │\n│ tag_id: 91732           │ longitude: 24.58    │ 14:00:00    │\n│                         │ vegetation: 0.96    │             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n**1. Measurement** : nom de la \"table\" (ex: `bird_migration`)\n\n**2. Tags** : métadonnées indexées\n   - Chaînes de caractères uniquement\n   - Cardinalité modérée recommandée (< 100k valeurs uniques)\n   - Utilisés pour filtrer : `WHERE bird_id = \"91732A\"`\n   - Exemples : bird_id, sensor_type, region\n\n**3. Fields** : valeurs mesurées\n   - Peuvent être de tout type (float, int, string, bool)\n   - Non indexés → ne pas filtrer dessus si possible\n   - Exemples : latitude, longitude, température, vitesse\n\n**4. Timestamp** : horodatage du point (nanoseconde precision)\n\nRègle d'or pour choisir tag vs field :\n- Tag si vous filtrez souvent dessus (`WHERE ...`)\n- Field si c'est une valeur mesurée/calculée\n- Tag seulement si cardinalité raisonnable (éviter timestamps, IDs uniques)"
  },
  {
   "cell_type": "markdown",
   "id": "f52af640",
   "metadata": {},
   "source": [
    "## Partie 1 : Installation et Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66859f60",
   "metadata": {},
   "source": [
    "### 1.1 Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# InfluxDB\n",
    "from influxdb_client import InfluxDBClient, Point\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "# Kaggle\n",
    "import kagglehub\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce743b0",
   "metadata": {},
   "source": [
    "### 1.2 Configuration InfluxDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2tmvlj3q645",
   "metadata": {},
   "source": "Configuration requise :\n- **URL** : adresse du serveur InfluxDB (ici dans Docker : `http://influxdb2:8086`)\n- **Token** : clé d'authentification (admin-token configuré dans docker-compose)\n- **Organisation** : espace de travail logique\n- **Bucket** : \"base de données\" où stocker les données"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2207e4e",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration InfluxDB\nINFLUX_URL = \"http://influxdb2:8086\" \nINFLUX_TOKEN = \"admin-token\"\nINFLUX_ORG = \"fil-A3-back-bigData\"\nINFLUX_BUCKET = \"animal-tracking\"  \n\n# Connexion au client\nclient = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)\nwrite_api = client.write_api(write_options=SYNCHRONOUS)\nquery_api = client.query_api()\n\nprint(\"Client InfluxDB connecté\")\nprint(f\"  Bucket: {INFLUX_BUCKET}\")\nprint(f\"  Organisation: {INFLUX_ORG}\")\n\n# Vérification de la connexion\ntry:\n    if client.ping():\n        print(\"Serveur InfluxDB répond\")\n    else:\n        print(\"Serveur ne répond pas\")\nexcept Exception as e:\n    print(f\"Erreur de connexion: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "2e568ede",
   "metadata": {},
   "source": "## Partie 2 : Chargement et Exploration des Données\n\nComprendre la structure des données et identifier ce qui en fait une série temporelle."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff07ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download the Movebank animal tracking dataset\n",
    "path = \"pulkit8595/movebank-animal-tracking\"\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "dataset_path = kagglehub.dataset_download(path)\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# List files in the dataset\n",
    "print(\"\\nFiles in dataset:\")\n",
    "for file in os.listdir(dataset_path):\n",
    "    file_size = os.path.getsize(os.path.join(dataset_path, file)) / 1024  # KB\n",
    "    print(f\"  - {file} ({file_size:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1243c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dataset_path, \"migration_original.csv\"))\n",
    "\n",
    "print(\"Dataset loaded into DataFrame\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations sur le dataset\n",
    "print(\"Informations sur le dataset:\")\n",
    "print(df.info())\n",
    "print(\"\\n Statistiques descriptives:\")\n",
    "print(df.describe())\n",
    "print(\"\\n Valeurs manquantes:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4o1aggor98p",
   "metadata": {},
   "source": "### 2.1 Analyse de cardinalité (crucial pour le design du schéma)\n\nLa cardinalité (nombre de valeurs distinctes) détermine si une colonne doit être un tag ou un field dans InfluxDB :\n- Cardinalité basse (< 100 valeurs) → Bon candidat pour tag\n- Cardinalité moyenne (100-100k valeurs) → Acceptable comme tag si filtrage fréquent\n- Cardinalité haute (> 100k valeurs) → NE PAS mettre en tag (explosion des index)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nr2x2vl0dgs",
   "metadata": {},
   "outputs": [],
   "source": "# Analyse de cardinalité pour chaque colonne\nprint(\"=\" * 70)\nprint(\"ANALYSE DE CARDINALITÉ\".center(70))\nprint(\"=\" * 70)\nprint(f\"{'Colonne':<50} {'Cardinalité':>15}\")\nprint(\"-\" * 70)\n\nfor col in df.columns:\n    cardinality = df[col].nunique()\n    total = len(df)\n    percentage = (cardinality / total) * 100\n    \n    # Indicateur selon le type de cardinalité\n    if cardinality == 1:\n        indicator = \"CONSTANTE\"\n    elif cardinality < 100:\n        indicator = \"BASSE (bon tag)\"\n    elif cardinality < 10000:\n        indicator = \"MOYENNE (tag ok)\"\n    elif cardinality == total:\n        indicator = \"UNIQUE (PK)\"\n    else:\n        indicator = \"HAUTE (field)\"\n    \n    print(f\"{col:<50} {cardinality:>10,} ({percentage:>5.1f}%)  {indicator}\")\n\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "id": "p21yel8d33i",
   "metadata": {},
   "source": "### 2.2 Analyse temporelle\n\nÀ quelle fréquence les oiseaux sont-ils trackés ? Est-ce régulier ?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h5iy4sqaar",
   "metadata": {},
   "outputs": [],
   "source": "df['timestamp'] = pd.to_datetime(df['timestamp'])\n\nprint(\"=\" * 70)\nprint(\"ANALYSE TEMPORELLE\".center(70))\nprint(\"=\" * 70)\nprint(f\"Date de début    : {df['timestamp'].min()}\")\nprint(f\"Date de fin      : {df['timestamp'].max()}\")\nprint(f\"Durée totale     : {(df['timestamp'].max() - df['timestamp'].min()).days} jours\")\nprint(f\"Nombre de points : {len(df):,}\")\nprint()\n\n# Analyser la fréquence de tracking\nsample_bird = df['individual-local-identifier'].value_counts().index[0]\nbird_data = df[df['individual-local-identifier'] == sample_bird].sort_values('timestamp')\nintervals = bird_data['timestamp'].diff().dt.total_seconds() / 3600\n\nprint(f\"Fréquence de tracking (oiseau exemple: {sample_bird}):\")\nprint(\"-\" * 70)\nprint(f\"  Intervalle moyen : {intervals.mean():.1f} heures\")\nprint(f\"  Intervalle médian: {intervals.median():.1f} heures\")\nprint(f\"  Min / Max        : {intervals.min():.1f}h / {intervals.max():.1f}h\")\nprint()\nprint(\"Tracking irrégulier (typique des données GPS réelles)\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "id": "exxx1uusr9w",
   "metadata": {},
   "source": "### 2.3 Distribution des données par oiseau\n\nTous les oiseaux sont-ils trackés de manière équivalente ?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xty9w4twmdk",
   "metadata": {},
   "outputs": [],
   "source": "# Distribution du nombre de points par oiseau\nbird_counts = df['individual-local-identifier'].value_counts()\n\nprint(\"=\" * 70)\nprint(\"DISTRIBUTION DU TRACKING PAR OISEAU\".center(70))\nprint(\"=\" * 70)\nprint(f\"Nombre d'oiseaux   : {len(bird_counts)}\")\nprint(f\"Points par oiseau  : {bird_counts.mean():.0f} ± {bird_counts.std():.0f} (moyenne ± écart-type)\")\nprint(f\"Min/Max            : {bird_counts.min()} / {bird_counts.max()} points\")\nprint()\nprint(\"Top 10 oiseaux les plus trackés:\")\nprint(\"-\" * 70)\nfor i, (bird_id, count) in enumerate(bird_counts.head(10).items(), 1):\n    bar = \"█\" * int(count / bird_counts.max() * 40)\n    print(f\"{i:2}. {bird_id:<15} {count:>5,} points {bar}\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "id": "m3ds8muxsyb",
   "metadata": {},
   "source": "### 2.4 Visualisation géographique rapide\n\nOù se trouvent les oiseaux ? Migration visible ?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4hntsmhw9cp",
   "metadata": {},
   "outputs": [],
   "source": "# Préparer les données\nsample_df = df.iloc[::5].copy()\nsample_df['date'] = pd.to_datetime(sample_df['timestamp'])\nsample_df['bird_id'] = sample_df['individual-local-identifier']\n\n\n# Visualisation des trajectoires d'oiseaux\nfig = px.scatter_geo(sample_df,\n                     lat='location-lat',\n                     lon='location-long',\n                     color='bird_id',\n                     hover_data=['date', 'bird_id'],\n                     title='Migration Patterns - Bird Tracking Data',\n                     projection='natural earth')\n\nfig.update_layout(\n    height=600,\n    showlegend=True,\n    geo=dict(\n        showland=True,\n        landcolor='rgb(243, 243, 243)',\n        coastlinecolor='rgb(204, 204, 204)',\n    )\n)\n\nfig.show()\n\n# Statistiques par oiseau\nprint(f\"Dataset échantillonné: {len(sample_df):,} points\")\nprint(f\"Nombre d'oiseaux: {sample_df['bird_id'].nunique()}\")\nprint(f\"Période: {sample_df['date'].min()} → {sample_df['date'].max()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "bd552f9a",
   "metadata": {},
   "source": "## Partie 3 : Nettoyage des Données\n\nPréparer les données pour InfluxDB en supprimant les colonnes inutiles."
  },
  {
   "cell_type": "markdown",
   "id": "7iz6k2zqkd9",
   "metadata": {},
   "source": "Stratégie de nettoyage :\n\n1. Supprimer les colonnes 100% vides (aucune valeur utile)\n   - Exemple : `manually-marked-outlier`, `NCEP NARR SFC Vegetation at Surface`\n   - Ces colonnes ajouteraient des fields inutiles dans InfluxDB\n\n2. Garder les colonnes redondantes mais utiles\n   - `visible` et `visible.1` semblent dupliquées mais gardons-les pour l'instant\n   - Si vraiment identiques, on pourra supprimer après vérification\n\n3. Renommer les colonnes pour la lisibilité\n   - `ECMWF Interim Full Daily...` → `vegetation_cover_low/high`\n   - Noms courts = queries Flux plus lisibles"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zlzucxigrhj",
   "metadata": {},
   "outputs": [],
   "source": "# Supprimer les colonnes 100% vides\ncolumns_to_drop = df.columns[df.isnull().all()].tolist()\ndf_clean = df.drop(columns=columns_to_drop)\n\n# Renommer les colonnes de végétation pour plus de clarté\ndf_clean = df_clean.rename(columns={\n    'ECMWF Interim Full Daily Invariant Low Vegetation Cover': 'vegetation_cover_low',\n    'ECMWF Interim Full Daily Invariant High Vegetation Cover': 'vegetation_cover_high'\n})\n\nprint(f\"Colonnes supprimées (100% vides): {len(columns_to_drop)}\")\nprint(f\"Colonnes renommées: vegetation_cover_low/high\")\nprint(f\"Dataset nettoyé: {df_clean.shape[0]:,} lignes × {df_clean.shape[1]} colonnes\")\n\n# Vérifier valeurs manquantes restantes\nnull_counts = df_clean.isnull().sum()\nremaining_nulls = null_counts[null_counts > 0]\nif len(remaining_nulls) > 0:\n    print(f\"\\nValeurs manquantes restantes:\")\n    for col, count in remaining_nulls.items():\n        print(f\"  {col}: {count:,} ({count/len(df_clean)*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "id": "45db26c1",
   "metadata": {},
   "source": "## Partie 4 : Design du Schéma et Insertion dans InfluxDB\n\nComprendre comment choisir entre tags et fields, puis insérer les données efficacement."
  },
  {
   "cell_type": "markdown",
   "id": "2jnu9w5r947",
   "metadata": {},
   "source": "### 4.1 Décisions de Design : Tags vs Fields\n\nRappel de la règle :\n- Tags = filtres fréquents + cardinalité raisonnable\n- Fields = valeurs mesurées + pas de filtrage\n\nAnalysons chaque colonne :"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "el8t5lm3qm5",
   "metadata": {},
   "outputs": [],
   "source": "# Matrice de décision Tag vs Field \nimport pandas as pd\n\ndecisions = [\n    {\n        \"Colonne\": \"timestamp\",\n        \"Cardinalité\": df_clean['timestamp'].nunique(),\n        \"Type\": \"TIMESTAMP\",\n        \"Décision\": \"Timestamp InfluxDB\",\n        \"Raison\": \"Index temporel principal\"\n    },\n    {\n        \"Colonne\": \"individual-local-identifier\",\n        \"Cardinalité\": df_clean['individual-local-identifier'].nunique(),\n        \"Type\": \"TAG\",\n        \"Décision\": \"Tag\",\n        \"Raison\": \"Filtrage fréquent par oiseau, ~199 valeurs (cardinalité ok)\"\n    },\n    {\n        \"Colonne\": \"tag-local-identifier\",\n        \"Cardinalité\": df_clean['tag-local-identifier'].nunique(),\n        \"Type\": \"TAG\",\n        \"Décision\": \"Tag\",\n        \"Raison\": \"Filtrage par device, ~199 valeurs (cardinalité ok)\"\n    },\n    {\n        \"Colonne\": \"sensor-type\",\n        \"Cardinalité\": df_clean['sensor-type'].nunique(),\n        \"Type\": \"FIELD\",\n        \"Décision\": \"Field (anti-pattern si tag)\",\n        \"Raison\": \"CARDINALITÉ = 1 ! Index inutile, gaspillage mémoire\"\n    },\n    {\n        \"Colonne\": \"individual-taxon-canonical-name\",\n        \"Cardinalité\": df_clean['individual-taxon-canonical-name'].nunique(),\n        \"Type\": \"FIELD\",\n        \"Décision\": \"Field\",\n        \"Raison\": \"Cardinalité = 1 (tous Larus fuscus), même raison\"\n    },\n    {\n        \"Colonne\": \"study-name\",\n        \"Cardinalité\": df_clean['study-name'].nunique(),\n        \"Type\": \"FIELD\",\n        \"Décision\": \"Field\",\n        \"Raison\": \"Cardinalité = 1, pas de filtrage nécessaire\"\n    },\n    {\n        \"Colonne\": \"event-id\",\n        \"Cardinalité\": df_clean['event-id'].nunique(),\n        \"Type\": \"FIELD\",\n        \"Décision\": \"Field\",\n        \"Raison\": \"Cardinalité = 89867 (unique), jamais en tag!\"\n    },\n    {\n        \"Colonne\": \"location-lat\",\n        \"Cardinalité\": df_clean['location-lat'].nunique(),\n        \"Type\": \"FIELD\",\n        \"Décision\": \"Field\",\n        \"Raison\": \"Valeur mesurée (coordonnée GPS)\"\n    },\n    {\n        \"Colonne\": \"location-long\",\n        \"Cardinalité\": df_clean['location-long'].nunique(),\n        \"Type\": \"FIELD\",\n        \"Décision\": \"Field\",\n        \"Raison\": \"Valeur mesurée (coordonnée GPS)\"\n    },\n    {\n        \"Colonne\": \"vegetation_cover_low\",\n        \"Cardinalité\": df_clean['vegetation_cover_low'].nunique(),\n        \"Type\": \"FIELD\",\n        \"Décision\": \"Field\",\n        \"Raison\": \"Valeur mesurée (donnée environnementale)\"\n    },\n    {\n        \"Colonne\": \"vegetation_cover_high\",\n        \"Cardinalité\": df_clean['vegetation_cover_high'].nunique(),\n        \"Type\": \"FIELD\",\n        \"Décision\": \"Field\",\n        \"Raison\": \"Valeur mesurée (donnée environnementale)\"\n    },\n    {\n        \"Colonne\": \"visible / visible.1\",\n        \"Cardinalité\": df_clean['visible'].nunique(),\n        \"Type\": \"FIELD\",\n        \"Décision\": \"Field\",\n        \"Raison\": \"Métadonnée booléenne, pas de filtrage\"\n    }\n]\n\ndecision_df = pd.DataFrame(decisions)\n\nprint(\"=\" * 100)\nprint(\"MATRICE DE DÉCISION : TAGS vs FIELDS\".center(100))\nprint(\"=\" * 100)\nprint(f\"{'Colonne':<35} {'Card.':>10} {'Type':^20} {'Raison':<30}\")\nprint(\"-\" * 100)\n\nfor _, row in decision_df.iterrows():\n    print(f\"{row['Colonne']:<35} {row['Cardinalité']:>10,} {row['Type']:^20} {row['Raison']:<30}\")\n\nprint(\"=\" * 100)\nprint()\nprint(\"RÉSUMÉ DU SCHÉMA:\")\nprint(f\"  Tags (2)   : individual-local-identifier, tag-local-identifier\")\nprint(f\"  Fields (9) : location-lat, location-long, event-id, vegetation_cover_*, etc.\")\nprint(f\"  Timestamp  : timestamp\")\nprint()"
  },
  {
   "cell_type": "markdown",
   "id": "2taaq8zbjun",
   "metadata": {},
   "source": "### 4.2 Anti-Patterns à Éviter\n\nNE PAS mettre en tags :\n- Colonnes avec cardinalité = 1 (constantes) → gaspillage mémoire\n- Colonnes avec haute cardinalité (> 100k) → explosion des index\n- Timestamps → utiliser le timestamp InfluxDB\n- Floats avec haute cardinalité → utiliser fields\n\nNotre schéma optimisé : 2 tags (~199 valeurs chacun), reste en fields"
  },
  {
   "cell_type": "markdown",
   "id": "rns0at7byus",
   "metadata": {},
   "source": [
    "### 4.3 Insertion des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3e016",
   "metadata": {},
   "outputs": [],
   "source": "df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])\n\nprint(f\"Insertion de {len(df_clean):,} records dans InfluxDB...\")\nprint(f\"Tags: individual-local-identifier, tag-local-identifier\")\nprint(f\"Fields: {df_clean.shape[1] - 3} colonnes\")\n\nimport time\nstart_time = time.time()\n\ntry:\n    write_api.write(\n        bucket=INFLUX_BUCKET, \n        org=INFLUX_ORG, \n        record=df_clean, \n        data_frame_measurement_name='bird_migration',\n        data_frame_tag_columns=[\n            'individual-local-identifier',\n            'tag-local-identifier',\n        ],\n        data_frame_timestamp_column='timestamp'\n    )\n    \n    elapsed = time.time() - start_time\n    print(f\"\\nInsertion réussie en {elapsed:.2f}s ({len(df_clean)/elapsed:.0f} points/s)\")\n    \nexcept Exception as e:\n    print(f\"Erreur: {e}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "id": "qe3v2yymgy",
   "metadata": {},
   "source": [
    "### 4.4 Vérification de l'Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4lw2p0ip4h",
   "metadata": {},
   "outputs": [],
   "source": "verification_query = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> group()\n  |> count()\n'''\n\nresult = query_api.query_data_frame(verification_query, org=INFLUX_ORG)\n\nif isinstance(result, list) and len(result) > 0:\n    num_fields = df_clean.shape[1] - 3\n    expected_records = len(df_clean)\n    \n    print(f\"Vérification:\")\n    print(f\"  Records insérés   : {expected_records:,}\")\n    print(f\"  Fields par record : {num_fields}\")\n    print(f\"  Taille estimée    : {(expected_records * num_fields * 8) / 1024 / 1024:.2f} MB (brut)\")\n    print(f\"  Avec compression  : ~{(expected_records * num_fields * 8) / 1024 / 1024 / 15:.2f} MB (ratio 15x)\")\nelse:\n    print(\"Aucune donnée trouvée\")"
  },
  {
   "cell_type": "markdown",
   "id": "63448e93",
   "metadata": {},
   "source": "## Partie 5 : Langage de Requêtes Flux - Construction Progressive\n\nMaîtriser Flux en construisant des requêtes de plus en plus complexes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee603b1",
   "metadata": {},
   "outputs": [],
   "source": "# Helper function pour afficher les résultats Flux proprement\ndef display_flux_result(result, title=\"Résultat\"):\n    \"\"\"Affiche les résultats d'une requête Flux de manière lisible\"\"\"\n    if isinstance(result, list):\n        if len(result) > 0:\n            result_df = pd.concat(result, ignore_index=True)\n            print(f\"{title}: {len(result_df)} lignes\")\n            display(result_df.head(10))\n            return result_df\n        else:\n            print(f\"{title}: Aucune donnée\")\n            return None\n    else:\n        print(f\"{title}: {len(result)} lignes\")\n        if len(result) > 0:\n            display(result.head(10))\n        return result\n\ndef process_flux_result(result):\n    \"\"\"Convertit le résultat Flux en DataFrame\"\"\"\n    if isinstance(result, list) and len(result) > 0:\n        return pd.concat(result, ignore_index=True)\n    return result if isinstance(result, pd.DataFrame) else None"
  },
  {
   "cell_type": "markdown",
   "id": "yj43zoxe83",
   "metadata": {},
   "source": "### 5.1 Requête de Base : Récupérer des Données\n\nAnatomie d'une requête Flux :\n```\nfrom(bucket: \"nom\")              ← Source des données\n  |> range(start: xxx, stop: yyy)  ← Filtre temporel (OBLIGATOIRE)\n  |> filter(fn: (r) => ...)        ← Filtres supplémentaires\n  |> limit(n: 10)                  ← Limiter le nombre de résultats\n```\n\nPipeline : Chaque opérateur `|>` passe les données à l'opérateur suivant"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86zs6l0lu9q",
   "metadata": {},
   "outputs": [],
   "source": "# Requête 1: Récupérer les 100 premiers points\nquery_basic = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 0)  \n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> limit(n: 100)\n'''\n\nprint(\"=\" * 80)\nprint(\"REQUÊTE 1: RÉCUPÉRATION BASIQUE (100 premiers points)\".center(80))\nprint(\"=\" * 80)\nprint(query_basic)\nprint()\n\nresult_basic = query_api.query_data_frame(query_basic, org=INFLUX_ORG)\ndf_basic = display_flux_result(result_basic, \"Requête basique\")"
  },
  {
   "cell_type": "markdown",
   "id": "fvqivilwe2f",
   "metadata": {},
   "source": "### 5.2 Filtrage par Tag : Requêtes Efficaces\n\nPourquoi filtrer sur les tags : Index optimisés → queries ultra-rapides\n\nÉquivalence :\n- Flux : `filter(fn: (r) => r[\"individual-local-identifier\"] == \"91732A\")`\n- SQL : `WHERE individual_local_identifier = '91732A'`\n- Pandas : `df[df['individual-local-identifier'] == '91732A']`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stz2u744psj",
   "metadata": {},
   "outputs": [],
   "source": "bird_id = \"91732A\"\n\nquery_by_bird = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"individual-local-identifier\"] == \"{bird_id}\")\n  |> limit(n: 50)\n'''\n\nprint(\"=\" * 80)\nprint(f\"REQUÊTE 2: FILTRAGE PAR TAG (oiseau {bird_id})\".center(80))\nprint(\"=\" * 80)\n\nresult_by_bird = query_api.query_data_frame(query_by_bird, org=INFLUX_ORG)\ndf_by_bird = display_flux_result(result_by_bird, f\"Données pour {bird_id}\")"
  },
  {
   "cell_type": "markdown",
   "id": "2mxs5iwpu8y",
   "metadata": {},
   "source": "### 5.3 Filtrage Temporel : La Force des Time-Series DB\n\nC'est ici que InfluxDB brille : requêtes temporelles ultra-optimisées\n\nExemple : Trouver les données de migration estivale (juin-août 2009)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yyt29jonjpi",
   "metadata": {},
   "outputs": [],
   "source": "query_summer = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 2009-06-01T00:00:00Z, stop: 2009-08-31T23:59:59Z)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"_field\"] == \"location-lat\")\n  |> limit(n: 100)\n'''\n\nprint(\"=\" * 80)\nprint(\"REQUÊTE 3: FILTRAGE TEMPOREL (été 2009)\".center(80))\nprint(\"=\" * 80)\n\nresult_summer = query_api.query_data_frame(query_summer, org=INFLUX_ORG)\ndf_summer = display_flux_result(result_summer, \"Données été 2009\")"
  },
  {
   "cell_type": "markdown",
   "id": "qdt2tp6cm6",
   "metadata": {},
   "source": "### 5.4 Restructuration avec Pivot : Transformer Fields en Colonnes\n\nProblème : InfluxDB retourne 1 ligne par field → difficile à analyser\n\nSolution : `pivot()` transforme les fields en colonnes (comme un DataFrame)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l8k9y6oe32h",
   "metadata": {},
   "outputs": [],
   "source": "query_pivot = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"individual-local-identifier\"] == \"91732A\")\n  |> filter(fn: (r) => r[\"_field\"] == \"location-lat\" or r[\"_field\"] == \"location-long\")\n  |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n  |> limit(n: 20)\n'''\n\nprint(\"=\" * 80)\nprint(\"REQUÊTE 4: PIVOT (fields → colonnes)\".center(80))\nprint(\"=\" * 80)\n\nresult_pivot = query_api.query_data_frame(query_pivot, org=INFLUX_ORG)\ndf_pivot = display_flux_result(result_pivot, \"Données pivotées\")"
  },
  {
   "cell_type": "markdown",
   "id": "ozincr4ck5b",
   "metadata": {},
   "source": "### 5.5 Agrégation : Compter, Grouper, Analyser\n\nOpérations d'agrégation courantes :\n- `count()` : nombre de points\n- `mean()` : moyenne\n- `sum()` : somme\n- `max()` / `min()` : valeurs extrêmes\n- `group()` : grouper par tag(s)\n\nExemple : Compter le nombre de tracking points par oiseau"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7yaew54pbrc",
   "metadata": {},
   "outputs": [],
   "source": "query_count = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"_field\"] == \"location-lat\")\n  |> group(columns: [\"individual-local-identifier\"])\n  |> count()\n  |> sort(columns: [\"_value\"], desc: true)\n  |> limit(n: 20)\n'''\n\nprint(\"=\" * 80)\nprint(\"REQUÊTE 5: AGRÉGATION (count par oiseau)\".center(80))\nprint(\"=\" * 80)\n\nresult_count = query_api.query_data_frame(query_count, org=INFLUX_ORG)\ndf_count = display_flux_result(result_count, \"Comptage par oiseau\")"
  },
  {
   "cell_type": "markdown",
   "id": "wporpri0qe",
   "metadata": {},
   "source": "### 5.6 Window Functions : Agrégation Temporelle\n\nCas d'usage : Calculer des moyennes par jour/heure/mois\n\n`aggregateWindow()` divise le temps en fenêtres et agrège chacune séparément"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glaesy6fgrk",
   "metadata": {},
   "outputs": [],
   "source": "query_window = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 2009-05-01T00:00:00Z, stop: 2009-05-31T23:59:59Z)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"_field\"] == \"vegetation_cover_high\")\n  |> aggregateWindow(every: 1d, fn: mean, createEmpty: false)\n  |> limit(n: 31)\n'''\n\nprint(\"=\" * 80)\nprint(\"REQUÊTE 6: WINDOW FUNCTION (moyenne journalière)\".center(80))\nprint(\"=\" * 80)\n\nresult_window = query_api.query_data_frame(query_window, org=INFLUX_ORG)\ndf_window = display_flux_result(result_window, \"Moyenne quotidienne végétation\")"
  },
  {
   "cell_type": "markdown",
   "id": "465d343a",
   "metadata": {},
   "source": "## Partie 5.5 : Analyse Time-Series - Patterns de Migration\n\nUtiliser Flux pour extraire des insights sur les migrations d'oiseaux."
  },
  {
   "cell_type": "markdown",
   "id": "fwe99qprqqu",
   "metadata": {},
   "source": "### 5.5.1 Trouver les Points Extrêmes de Migration\n\nQuelle est la latitude la plus au nord et au sud visitée par chaque oiseau ?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o8q2wsxklw",
   "metadata": {},
   "outputs": [],
   "source": "query_extremes = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"_field\"] == \"location-lat\")\n  |> group(columns: [\"individual-local-identifier\"])\n  |> reduce(\n      fn: (r, accumulator) => ({{\n        bird_id: r[\"individual-local-identifier\"],\n        max_lat: if r._value > accumulator.max_lat then r._value else accumulator.max_lat,\n        min_lat: if r._value < accumulator.min_lat then r._value else accumulator.min_lat\n      }}),\n      identity: {{bird_id: \"\", max_lat: -90.0, min_lat: 90.0}}\n    )\n  |> limit(n: 10)\n'''\n\nresult_extremes = query_api.query_data_frame(query_extremes, org=INFLUX_ORG)\ndf_extremes = display_flux_result(result_extremes, \"Points extrêmes par oiseau\")\n\nif df_extremes is not None and len(df_extremes) > 0:\n    if 'max_lat' in df_extremes.columns and 'min_lat' in df_extremes.columns:\n        df_extremes['migration_range'] = df_extremes['max_lat'] - df_extremes['min_lat']\n        print(\"\\nTop 3 amplitudes de migration:\")\n        top_migrants = df_extremes.nlargest(3, 'migration_range')[['bird_id', 'min_lat', 'max_lat', 'migration_range']]\n        print(top_migrants.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "e614rauiies",
   "metadata": {},
   "source": "### 5.5.3 Analyse Saisonnière : Été vs Hiver\n\nHypothèse : Les oiseaux migrateurs se déplacent vers le nord en été (reproduction) et vers le sud en hiver\n\nTestons cette hypothèse avec les données."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "um7h5rsu39b",
   "metadata": {},
   "outputs": [],
   "source": "query_summer_lat = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 2009-06-01T00:00:00Z, stop: 2009-08-31T23:59:59Z)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"_field\"] == \"location-lat\")\n  |> mean()\n'''\n\nquery_winter_lat = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 2009-12-01T00:00:00Z, stop: 2010-02-28T23:59:59Z)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"_field\"] == \"location-lat\")\n  |> mean()\n'''\n\nresult_summer = query_api.query_data_frame(query_summer_lat, org=INFLUX_ORG)\nresult_winter = query_api.query_data_frame(query_winter_lat, org=INFLUX_ORG)\n\ndef extract_mean(result):\n    if isinstance(result, list) and len(result) > 0:\n        return result[0]['_value'].values[0] if '_value' in result[0].columns else None\n    elif isinstance(result, pd.DataFrame) and len(result) > 0:\n        return result['_value'].values[0] if '_value' in result.columns else None\n    return None\n\nsummer_mean = extract_mean(result_summer)\nwinter_mean = extract_mean(result_winter)\n\nif summer_mean and winter_mean:\n    print(f\"Latitude moyenne:\")\n    print(f\"  Été 2009    : {summer_mean:.2f}°\")\n    print(f\"  Hiver 2009  : {winter_mean:.2f}°\")\n    print(f\"  Différence  : {summer_mean - winter_mean:.2f}°\")\n    print(f\"\\nLes oiseaux sont {summer_mean - winter_mean:.1f}° plus au nord en été\")\n    print(\"Pattern de migration classique (Scandinavie → Afrique)\")\nelse:\n    print(\"Données insuffisantes\")"
  },
  {
   "cell_type": "markdown",
   "id": "u197ftdd1co",
   "metadata": {},
   "source": "### 5.5.4 Efficacité du Stockage : Compression des Time-Series\n\nInfluxDB utilise une compression très efficace pour les données time-series.\n\nPourquoi c'est important :\n- Données time-series = souvent répétitives (même tags, valeurs proches)\n- Compression 10-20x typique → économies massives de stockage\n- Impact sur coûts cloud et performance I/O"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0nzfv6rjgbck",
   "metadata": {},
   "outputs": [],
   "source": "num_records = len(df_clean)\nnum_fields = df_clean.shape[1] - 3\nbytes_per_record = 148  # timestamp(8) + tags(40) + fields(80) + overhead(20)\n\ntotal_size_mb = (num_records * bytes_per_record) / 1024 / 1024\n\nprint(\"EFFICACITÉ DU STOCKAGE\")\nprint(\"=\" * 60)\nprint(f\"Records          : {num_records:,}\")\nprint(f\"Points totaux    : {num_records * num_fields:,}\")\nprint(f\"Taille brute     : {total_size_mb:.2f} MB\")\nprint(f\"Avec compression : {total_size_mb / 15:.2f} MB (ratio 15x)\")\nprint()\nprint(\"Compression efficace grâce à:\")\nprint(\"  - Tags répétés (mêmes oiseaux)\")\nprint(\"  - Timestamps ordonnés (delta encoding)\")\nprint(\"  - Coordonnées GPS proches (peu de variation)\")\nprint(\"  - Colonnes constantes (run-length encoding)\")"
  },
  {
   "cell_type": "markdown",
   "id": "x07s9o6n2q",
   "metadata": {},
   "source": "### 5.5.5 Retention Policies : Gestion du Cycle de Vie des Données\n\nEn production, on ne garde pas toutes les données éternellement.\n\nRetention Policy = règle qui définit :\n- Combien de temps garder les données\n- Suppression automatique des données anciennes\n- Économie de stockage et amélioration des performances"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svqakq3jcw",
   "metadata": {},
   "outputs": [],
   "source": "print(\"RETENTION POLICIES\")\nprint(\"=\" * 60)\nprint()\nprint(\"Cas d'usage typiques:\")\nprint(\"  1. Données brutes (7 jours) - tracking temps réel\")\nprint(\"  2. Agrégations horaires (90 jours) - dashboards\")\nprint(\"  3. Moyennes journalières (illimité) - analyses long terme\")\nprint()\nprint(\"Configuration exemple:\")\nprint(\"  influx bucket create --name raw-tracking \\\\\")\nprint(\"    --retention 720h --org fil-A3-back-bigData\")\nprint()\nprint(\"Bénéfices:\")\nprint(\"  - Contrôle automatique de la croissance\")\nprint(\"  - Queries plus rapides (moins de données)\")\nprint(\"  - Coûts prévisibles\")"
  },
  {
   "cell_type": "markdown",
   "id": "ajn37athxa",
   "metadata": {},
   "source": "## Partie 6 : Visualisations - Démonstration (OPTIONNEL)\n\nNote : Cette partie est une démonstration optionnelle pour montrer ce qu'on peut faire avec les données. L'objectif principal du TP est d'apprendre InfluxDB (schema design, Flux queries), pas la visualisation de données.\n\nMontrer rapidement quelques visualisations possibles des patterns de migration."
  },
  {
   "cell_type": "markdown",
   "id": "cixzzzysn25",
   "metadata": {},
   "source": "### 6.1 Carte Interactive des Routes de Migration\n\nVisualiser les trajectoires des 3 oiseaux les plus trackés sur une carte interactive avec Folium."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ftsurtfmh8",
   "metadata": {},
   "outputs": [],
   "source": "query_top_birds = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"_field\"] == \"location-lat\")\n  |> group(columns: [\"individual-local-identifier\"])\n  |> count()\n  |> sort(columns: [\"_value\"], desc: true)\n  |> limit(n: 3)\n'''\n\nresult_top = query_api.query_data_frame(query_top_birds, org=INFLUX_ORG)\ndf_top = process_flux_result(result_top)\ntop_bird_ids = df_top['individual-local-identifier'].tolist()\n\nprint(f\"Top 3 oiseaux: {', '.join(top_bird_ids)}\")\n\nm = folium.Map(location=[50, 15], zoom_start=4, tiles='OpenStreetMap')\ncolors = ['red', 'blue', 'green']\n\nfor idx, bird_id in enumerate(top_bird_ids):\n    query_traj = f'''\n    from(bucket: \"{INFLUX_BUCKET}\")\n      |> range(start: 0)\n      |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n      |> filter(fn: (r) => r[\"individual-local-identifier\"] == \"{bird_id}\")\n      |> filter(fn: (r) => r[\"_field\"] == \"location-lat\" or r[\"_field\"] == \"location-long\")\n      |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n      |> sort(columns: [\"_time\"])\n    '''\n    \n    result_traj = query_api.query_data_frame(query_traj, org=INFLUX_ORG)\n    df_traj = process_flux_result(result_traj)\n    \n    if df_traj is not None and len(df_traj) > 0:\n        points = list(zip(df_traj['location-lat'], df_traj['location-long']))\n        \n        from folium.plugins import AntPath\n        AntPath(points, color=colors[idx], weight=2.5, opacity=0.8, delay=800,\n                popup=f\"<b>{bird_id}</b><br>{len(points)} points\").add_to(m)\n        \n        folium.Marker(points[0], popup=f\"Départ: {bird_id}\",\n                     icon=folium.Icon(color=colors[idx], icon='play')).add_to(m)\n        folium.Marker(points[-1], popup=f\"Fin: {bird_id}\",\n                     icon=folium.Icon(color=colors[idx], icon='stop')).add_to(m)\n\ndisplay(m)"
  },
  {
   "cell_type": "markdown",
   "id": "9xzsccgjco6",
   "metadata": {},
   "source": "### 6.2 Time-Series: Latitude au Fil du Temps\n\nVisualiser le pattern de migration saisonnière d'un oiseau (latitude vs temps)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ajgt7ekcv1w",
   "metadata": {},
   "outputs": [],
   "source": "main_bird = top_bird_ids[0]\n\nquery_lat_time = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"individual-local-identifier\"] == \"{main_bird}\")\n  |> filter(fn: (r) => r[\"_field\"] == \"location-lat\")\n  |> sort(columns: [\"_time\"])\n'''\n\nresult_lat = query_api.query_data_frame(query_lat_time, org=INFLUX_ORG)\ndf_lat = process_flux_result(result_lat)\n\nif df_lat is not None and len(df_lat) > 0:\n    df_lat['_time'] = pd.to_datetime(df_lat['_time'])\n    \n    fig, ax = plt.subplots(figsize=(16, 6))\n    ax.plot(df_lat['_time'], df_lat['_value'], linewidth=1.5, color='#2E86AB', alpha=0.8)\n    \n    years = df_lat['_time'].dt.year.unique()\n    for year in years:\n        summer_start = pd.Timestamp(f'{year}-06-01')\n        summer_end = pd.Timestamp(f'{year}-08-31')\n        ax.axvspan(summer_start, summer_end, alpha=0.2, color='yellow', \n                   label='Été' if year == years[0] else '')\n        \n        winter_start = pd.Timestamp(f'{year}-12-01')\n        winter_end = pd.Timestamp(f'{year+1}-02-28')\n        ax.axvspan(winter_start, winter_end, alpha=0.2, color='lightblue',\n                   label='Hiver' if year == years[0] else '')\n    \n    ax.set_xlabel('Date', fontsize=12)\n    ax.set_ylabel('Latitude (°)', fontsize=12)\n    ax.set_title(f'Pattern de Migration: {main_bird}', fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n    ax.legend(loc='upper right')\n    \n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Aucune donnée\")"
  },
  {
   "cell_type": "markdown",
   "id": "a9vubp0b8e",
   "metadata": {},
   "source": "### 6.3 Heatmap: Densité des Points de Tracking en Fonction du Temps\n\nVisualiser l'évolution temporelle des zones de présence des oiseaux (migration saisonnière)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ow2qcxpa8h",
   "metadata": {},
   "outputs": [],
   "source": "query_all_points = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"_field\"] == \"location-lat\" or r[\"_field\"] == \"location-long\")\n  |> sample(n: 1000)\n  |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n'''\n\nresult_all = query_api.query_data_frame(query_all_points, org=INFLUX_ORG)\ndf_all = process_flux_result(result_all)\n\nif df_all is not None and len(df_all) > 0:\n    from folium.plugins import HeatMap\n    \n    m_heat = folium.Map(location=[45, 15], zoom_start=4, tiles='CartoDB positron')\n    heat_data = [[row['location-lat'], row['location-long']] for idx, row in df_all.iterrows()]\n    \n    HeatMap(heat_data, radius=8, blur=10, max_zoom=6,\n            gradient={0.4: 'blue', 0.65: 'lime', 0.8: 'yellow', 1.0: 'red'}).add_to(m_heat)\n    \n    print(f\"Heatmap: {len(heat_data):,} points\")\n    display(m_heat)\nelse:\n    print(\"Aucune donnée\")"
  },
  {
   "cell_type": "markdown",
   "id": "qwt36u3q1w9",
   "metadata": {},
   "source": "### 6.4 Corrélation: Latitude vs Couverture Végétale\n\nY a-t-il une corrélation entre la latitude (zones géographiques) et la végétation ?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98mqpz76h",
   "metadata": {},
   "outputs": [],
   "source": "query_veg = f'''\nfrom(bucket: \"{INFLUX_BUCKET}\")\n  |> range(start: 0)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"bird_migration\")\n  |> filter(fn: (r) => r[\"_field\"] == \"vegetation_cover_high\" or r[\"_field\"] == \"vegetation_cover_low\" or r[\"_field\"] == \"location-lat\")\n  |> sample(n: 1000)\n  |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n'''\n\nresult_veg = query_api.query_data_frame(query_veg, org=INFLUX_ORG)\ndf_veg = process_flux_result(result_veg)\n\nif df_veg is not None and len(df_veg) > 0 and 'vegetation_cover_high' in df_veg.columns:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    ax1.scatter(df_veg['location-lat'], df_veg['vegetation_cover_high'], \n                alpha=0.3, s=2, color='#2E7D32')\n    ax1.set_xlabel('Latitude (°)', fontsize=11)\n    ax1.set_ylabel('Haute Végétation', fontsize=11)\n    ax1.set_title('Latitude vs Haute Végétation', fontsize=13, fontweight='bold')\n    ax1.grid(True, alpha=0.3)\n    \n    corr_high = df_veg['location-lat'].corr(df_veg['vegetation_cover_high'])\n    ax1.text(0.05, 0.95, f'Corr: {corr_high:.3f}', transform=ax1.transAxes, \n             fontsize=10, verticalalignment='top',\n             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    if 'vegetation_cover_low' in df_veg.columns:\n        ax2.scatter(df_veg['location-lat'], df_veg['vegetation_cover_low'], \n                    alpha=0.3, s=2, color='#F57C00')\n        ax2.set_xlabel('Latitude (°)', fontsize=11)\n        ax2.set_ylabel('Basse Végétation', fontsize=11)\n        ax2.set_title('Latitude vs Basse Végétation', fontsize=13, fontweight='bold')\n        ax2.grid(True, alpha=0.3)\n        \n        corr_low = df_veg['location-lat'].corr(df_veg['vegetation_cover_low'])\n        ax2.text(0.05, 0.95, f'Corr: {corr_low:.3f}', transform=ax2.transAxes,\n                 fontsize=10, verticalalignment='top',\n                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Données non disponibles\")"
  },
  {
   "cell_type": "markdown",
   "id": "ku8ob4a05w",
   "metadata": {},
   "source": "## Fin du TP : Récapitulatif\n\n### Ce que nous avons appris :\n\nConcepts Time-Series :\n- Différence entre bases relationnelles, documents, et time-series\n- Pourquoi InfluxDB pour le tracking GPS et les métriques\n\nSchema Design InfluxDB :\n- Tags vs Fields : la décision la plus critique\n- Impact de la cardinalité sur les performances\n- Anti-patterns à éviter (tags constants, haute cardinalité)\n\nLangage Flux :\n- Requêtes basiques (`from`, `range`, `filter`)\n- Agrégations (`count`, `mean`, `group`)\n- Window functions pour downsampling\n- Pivot pour restructurer les données\n\nAnalyse Time-Series :\n- Requêtes pour trouver les extremes (min/max)\n- Détection de patterns saisonniers avec agrégation temporelle\n- Analyse comparative (été vs hiver)\n\nOptimisation & Gestion :\n- Retention policies pour la gestion du cycle de vie des données\n- Compression et efficacité du stockage\n- Impact du schema design sur les performances\n\nVisualisations (Démonstration) :\n- Cartes interactives de migration\n- Time-series plots\n- Heatmaps de densité\n- Analyse de corrélation\n\n### InfluxDB vs MongoDB : Quand utiliser quoi ?\n\n| Critère | InfluxDB | MongoDB |\n|---------|----------|---------|\n| Use case | Métriques, IoT, tracking temporel | Documents flexibles, applications web |\n| Structure | Tags + Fields + Timestamp | JSON documents |\n| Queries temporelles | Ultra-rapides | Correct avec indexes |\n| Compression | 10-20x | Moyenne |\n| Flexibilité schéma | Tags/Fields fixes | Total freedom |\n| Relations complexes | Pas adapté | Avec $lookup |\n\nChoix pour notre projet :\n- InfluxDB : données temporelles, tracking GPS, métriques environnementales\n- MongoDB : aurait fonctionné mais moins efficace pour les requêtes temporelles\n\n### Prochaines étapes suggérées :\n\n1. Continuous Queries : Pré-calculer des agrégations (downsampling automatique)\n2. Tasks : Automatiser le traitement de données\n3. Alerting : Détecter des patterns anormaux (oiseau hors zone attendue)\n4. Grafana : Dashboards temps-réel pour monitoring\n5. Sharding & Replication : Scalabilité horizontale pour très gros volumes\n\nRessources :\n- [Documentation InfluxDB](https://docs.influxdata.com/influxdb/v2.0/)\n- [Flux Language Guide](https://docs.influxdata.com/flux/v0.x/)\n- [InfluxDB University](https://university.influxdata.com/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}